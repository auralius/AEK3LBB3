<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Demo-1: Linear Classifiers with Arduino &#8212; AEK3LBB3</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/traditional.css?v=608ddc6c" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Demo-2: Two-Layer Fully Connected Network with Arduino" href="arduino_mlp_classifier.html" />
    <link rel="prev" title="Convolutional Neural Network with Pre-Convolved Input" href="cnn.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="arduino_mlp_classifier.html" title="Demo-2: Two-Layer Fully Connected Network with Arduino"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="cnn.html" title="Convolutional Neural Network with Pre-Convolved Input"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="intro.html">AEK3LBB3</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Demo-1: Linear Classifiers with Arduino</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="demo-1-linear-classifiers-with-arduino">
<h1>Demo-1: Linear Classifiers with Arduino<a class="headerlink" href="#demo-1-linear-classifiers-with-arduino" title="Link to this heading">¶</a></h1>
<p>This is a demonstration for the linear classifiers (SVM and Softmax).</p>
<section id="arduino-code-repository">
<h2>Arduino Code repository<a class="headerlink" href="#arduino-code-repository" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://github.com/auralius/arduino-linear-classifier">https://github.com/auralius/arduino-linear-classifier</a></p>
<p>Check the folder with the following name: <code class="docutils literal notranslate"><span class="pre">usps_16by16</span></code>.</p>
<p><img alt="" src="_images/demo_16by16.gif" /></p>
</section>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Link to this heading">¶</a></h2>
<p>As usual, let’s start with some preparations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
    <span class="s2">&quot;text.usetex&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;font.family&quot;</span><span class="p">:</span> <span class="s2">&quot;sans-serif&quot;</span><span class="p">,</span>
    <span class="s2">&quot;font.size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">})</span>

<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Put the SVM class and the Softmax class here so that we can use them easily to train our classifier.</p>
<section id="the-svm-class">
<h3>The SVM class<a class="headerlink" href="#the-svm-class" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>

<span class="k">class</span> <span class="nc">SVM</span><span class="p">():</span>
    <span class="nd">@staticmethod</span>
    <span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fastmath</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">svm_loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Structured SVM loss function, naive implementation (with loops).</span>

<span class="sd">        Inputs have dimension D, there are C classes, and we operate on minibatches</span>
<span class="sd">        of N examples.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - W: A numpy array of shape (D+1, C) containing weights.</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing a minibatch of data.</span>
<span class="sd">        - Y: A numpy array of shape (N,) containing training labels; y[i] = c means</span>
<span class="sd">          that X[i] has label c, where 0 &lt;= c &lt; C.</span>
<span class="sd">        - reg: (float) regularization strength</span>

<span class="sd">        Returns a tuple of:</span>
<span class="sd">        - loss: loss as single float</span>
<span class="sd">        - dW: gradient with respect to weights W; an array of same shape as W</span>

<span class="sd">        References:</span>
<span class="sd">        - https://github.com/lightaime/cs231n</span>
<span class="sd">        - https://github.com/mantasu/cs231n</span>
<span class="sd">        - https://github.com/jariasf/CS231n</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))))</span> <span class="c1"># the last column is 1: to allow augmentation of bias vector into W</span>

        <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># initialize the gradient as zero</span>

        <span class="c1"># compute the loss and the gradient</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@W</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">Fi</span> <span class="o">=</span> <span class="n">F</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">Fyi</span> <span class="o">=</span> <span class="n">Fi</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="n">Li</span> <span class="o">=</span> <span class="n">Fi</span> <span class="o">-</span> <span class="n">Fyi</span> <span class="o">+</span> <span class="mi">1</span>        <span class="c1"># </span>
            <span class="n">Li</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Li</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># max(0, ...)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">Li</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>         
                <span class="k">if</span> <span class="n">Li</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                    <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>    <span class="c1"># update gradient for incorrect label</span>
                    <span class="n">dW</span><span class="p">[:,</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># update gradient for correct label</span>

        <span class="n">loss</span>  <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># add regularization to the loss.</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span> <span class="c1"># append partial derivative of regularization term</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Structured SVM loss function, naive implementation (with loops).</span>

<span class="sd">        Inputs have dimension D, there are C classes, and we operate on minibatches</span>
<span class="sd">        of N examples.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - W: A numpy array of shape (D+1, C) containing weights.</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing a minibatch of data.</span>
<span class="sd">        - Y: A numpy array of shape (N,) containing training labels; y[i] = c means</span>
<span class="sd">          that X[i] has label c, where 0 &lt;= c &lt; C.</span>
<span class="sd">        - reg: (float) regularization strength</span>

<span class="sd">        Returns a tuple of:</span>
<span class="sd">        - loss: loss as single float</span>
<span class="sd">        - dW: gradient with respect to weights W; an array of same shape as W</span>

<span class="sd">        References:</span>
<span class="sd">        - https://github.com/lightaime/cs231n</span>
<span class="sd">        - https://github.com/mantasu/cs231n</span>
<span class="sd">        - https://github.com/jariasf/CS231n</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))))</span> <span class="c1"># the last column is 1: to allow augmentation of bias vector into W</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># scores</span>
        <span class="n">F</span>  <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">W</span>

        <span class="c1"># Scores for correct labels</span>
        <span class="n">Fy</span> <span class="o">=</span> <span class="n">F</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># compute the loss</span>
        <span class="n">L</span>  <span class="o">=</span> <span class="n">F</span> <span class="o">-</span> <span class="n">Fy</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">L</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">a_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># max(0, ...)</span>
        <span class="n">L_</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># keep the original</span>
        <span class="n">L</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="p">,</span> <span class="n">Y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># exclude the correct labels</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">L</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>

        <span class="c1"># compute the gradient</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="n">L_</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="c1"># positive for incorrect labels</span>
        <span class="n">dW</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span> <span class="o">=</span> <span class="n">dW</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span> <span class="o">-</span> <span class="n">dW</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># negative for correct labels</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dW</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>  <span class="c1"># gradient with respect to W</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dW</span><span class="p">)</span>
      

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Train this linear classifier using stochastic gradient descent. </span>
<span class="sd">        Setting the &#39;batch_size=None&quot; turns of the stochastic feature.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing training data; there are N</span>
<span class="sd">          training samples each of dimension D.</span>
<span class="sd">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span>
<span class="sd">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span>
<span class="sd">        - learning_rate: (float) learning rate for optimization.</span>
<span class="sd">        - reg: (float) regularization strength.</span>
<span class="sd">        - num_iters: (integer) number of steps to take when optimizing</span>
<span class="sd">        - batch_size: (integer) number of training examples to use at each step.</span>
<span class="sd">        - verbose: (boolean) If true, print progress during optimization.</span>
<span class="sd">        - verbose_steps: (integer) print proress once every verbose_steps</span>

<span class="sd">        Outputs:</span>
<span class="sd">        A list containing the value of the loss function at each training iteration.</span>
<span class="sd">        &#39;&#39;&#39;</span>
          
        <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">C</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> 
        
        <span class="c1"># lazily initialize W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># dim+1, to bias vector is augmented into W </span>

        <span class="c1"># Run stochastic gradient descent to optimize W</span>
        <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Sample batch_size elements from the training data and their           </span>
            <span class="c1"># corresponding labels to use in this round of gradient descent.        </span>
            <span class="c1"># Store the data in X_batch and their corresponding labels in           </span>
            <span class="c1"># y_batch; after sampling X_batch should have shape (dim, batch_size)   </span>
            <span class="c1"># and y_batch should have shape (batch_size,)                           </span>
            
            <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">y_batch</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">svm_loss_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">svm_loss_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
            
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Update the weights using the gradient and the learning rate.          </span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
     
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">it</span> <span class="o">%</span> <span class="n">verbose_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">it</span> <span class="o">==</span> <span class="n">num_iters</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;iteration </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1">: loss </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">loss</span><span class="p">),</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_history</span>
    

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use the trained weights of this linear classifier to predict labels for</span>
<span class="sd">        data points.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing training data; there are N</span>
<span class="sd">          training samples each of dimension D.</span>

<span class="sd">        Returns:</span>
<span class="sd">        - Y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span>
<span class="sd">          array of length N, and each element is an integer giving the predicted</span>
<span class="sd">          class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">X</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))))</span>

        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Y_pred</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-softmax-class">
<h3>The Softmax class<a class="headerlink" href="#the-softmax-class" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>

<span class="k">class</span> <span class="nc">Softmax</span><span class="p">():</span>
    <span class="nd">@staticmethod</span>
    <span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fastmath</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs have dimension D, there are C classes, and we operate on minibatches</span>
<span class="sd">        of N examples.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - W: A numpy array of shape (D+1, C) containing weights.</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing a minibatch of data.</span>
<span class="sd">        - Y: A numpy array of shape (N,) containing training labels; Y[i] = c means</span>
<span class="sd">          that X[i] has label c, where 0 &lt;= c &lt; C.</span>
<span class="sd">        - reg: (float) regularization strength</span>

<span class="sd">        Returns a tuple of:</span>
<span class="sd">        - loss: loss as single float</span>
<span class="sd">        - dW: gradient with respect to weights W; an array of same shape as W</span>

<span class="sd">        References:</span>
<span class="sd">        - https://github.com/lightaime/cs231n</span>
<span class="sd">        - https://github.com/mantasu/cs231n</span>
<span class="sd">        - https://github.com/jariasf/CS231n</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize the loss and gradient to zero.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">dW</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        
        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># samples </span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># classes</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span> <span class="c1"># the last column is 1: to allow augmentation of bias vector into W</span>

        <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@W</span> 

        <span class="c1"># Softmax Loss</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">Fi</span> <span class="o">=</span> <span class="n">F</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">F</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">expFi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Fi</span><span class="p">)</span>
            <span class="n">softmax</span> <span class="o">=</span> <span class="n">expFi</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">expFi</span><span class="p">)</span> 
            <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>

            <span class="c1"># Weight Gradients</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
                <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">softmax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">dW</span><span class="p">[:,</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Average and regulation</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span>  <span class="n">N</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">W</span> 

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
    
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">softmax_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs have dimension D, there are C classes, and we operate on minibatches</span>
<span class="sd">        of N examples.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - W: A numpy array of shape (D+1, C) containing weights.</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing a minibatch of data.</span>
<span class="sd">        - Y: A numpy array of shape (N,) containing training labels; Y[i] = c means</span>
<span class="sd">          that X[i] has label c, where 0 &lt;= c &lt; C.</span>
<span class="sd">        - reg: (float) regularization strength</span>

<span class="sd">        Returns a tuple of:</span>
<span class="sd">        - loss: loss as single float</span>
<span class="sd">        - dW: gradient with respect to weights W; an array of same shape as W</span>

<span class="sd">        References:</span>
<span class="sd">        - https://github.com/lightaime/cs231n</span>
<span class="sd">        - https://github.com/mantasu/cs231n</span>
<span class="sd">        - https://github.com/jariasf/CS231n</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize the loss and gradient to zero.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">dW</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        
        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># samples </span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># classes</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span> <span class="c1"># the last column is 1: to allow augmentation of bias vector into W</span>

        <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@W</span> 
        
        <span class="c1"># Softmax Loss</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">F</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">expF</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">F</span><span class="p">)</span>
        <span class="n">softmax</span> <span class="o">=</span> <span class="n">expF</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">expF</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">Y</span><span class="p">]))</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>

        <span class="c1"># derivative of the loss</span>
        <span class="n">softmax</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>                  <span class="c1"># update the softmax table</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">softmax</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>       <span class="c1"># calculate gradient</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>

    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Train this linear classifier using stochastic gradient descent.</span>
<span class="sd">        Setting the &#39;batch_size=None&quot; turns of the stochastic feature.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing training data; there are N</span>
<span class="sd">          training samples each of dimension D.</span>
<span class="sd">        - Y: A numpy array of shape (N,) containing training labels; y[i] = c</span>
<span class="sd">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span>
<span class="sd">        - learning_rate: (float) learning rate for optimization.</span>
<span class="sd">        - reg: (float) regularization strength.</span>
<span class="sd">        - num_iters: (integer) number of steps to take when optimizing</span>
<span class="sd">        - batch_size: (integer) number of training examples to use at each step.</span>
<span class="sd">        - verbose: (boolean) If true, print progress during optimization.</span>
<span class="sd">        - verbose_steps: (integer) print proress once every verbose_steps</span>

<span class="sd">        Outputs:</span>
<span class="sd">        A list containing the value of the loss function at each training iteration.</span>
<span class="sd">        &#39;&#39;&#39;</span>
          
        <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">C</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> 
        
        <span class="c1"># lazily initialize W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># dim+1, to bias vector is augmented into W </span>

        <span class="c1"># Run stochastic gradient descent to optimize W</span>
        <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Sample batch_size elements from the training data and their           </span>
            <span class="c1"># corresponding labels to use in this round of gradient descent.        </span>
            <span class="c1"># Store the data in X_batch and their corresponding labels in           </span>
            <span class="c1"># y_batch; after sampling X_batch should have shape (dim, batch_size)   </span>
            <span class="c1"># and y_batch should have shape (batch_size,)                           </span>
            
            <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
              <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
              <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
              <span class="n">y_batch</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
              <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_loss_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_loss_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
            
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Update the weights using the gradient and the learning rate.          </span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
     
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">it</span> <span class="o">%</span> <span class="n">verbose_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">it</span> <span class="o">==</span> <span class="n">num_iters</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;iteration </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1">: loss </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">loss</span><span class="p">),</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_history</span>
    

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use the trained weights of this linear classifier to predict labels for</span>
<span class="sd">        data points.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - X: A numpy array of shape (N, D) containing training data; there are N</span>
<span class="sd">          training samples each of dimension D.</span>

<span class="sd">        Returns:</span>
<span class="sd">        - Y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span>
<span class="sd">          array of length N, and each element is an integer giving the predicted</span>
<span class="sd">          class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">X</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))))</span>

        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Y_pred</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="usps-dataset">
<h2>USPS Dataset<a class="headerlink" href="#usps-dataset" title="Link to this heading">¶</a></h2>
<p>The USPS dataset is a little more suitable for our implementation then the MNIST dataset. Images in the USPS dataset are wrapped as tightly as possible inside their bounding boxes. Thus, every image is centered at the center of its bounding box. As for images in the MNIST dataset, they are not tightly wrapped. Each image is centered its center of mass. Finding center of mass of an image is considerably very resource demanding for a general low-cost microcontoller that we will use.</p>
<p>The USPS dataset can be found here:</p>
<p>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">h5py</span>
<span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="s2">&quot;./datasets/usps/usps.h5&quot;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">hf</span><span class="p">:</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">hf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)[:]</span> <span class="o">*</span> <span class="mf">255.0</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">hf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">)[:])</span> 

    <span class="n">X_test</span> <span class="o">=</span> <span class="n">hf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)[:]</span> <span class="o">*</span> <span class="mf">255.0</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">hf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">)[:])</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training data&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dimension numbers :&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of data    :&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels            :&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y_train</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test data&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dimension numbers :&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of data    :&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels            :&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training data
Dimension numbers : 256
Number of data    : 7291
Labels            : [0 1 2 3 4 5 6 7 8 9]
Test data
Dimension numbers : 256
Number of data    : 2007
Labels            : [0 1 2 3 4 5 6 7 8 9]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">X_train_</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">X_train_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">255.0</span> <span class="o">-</span> <span class="n">X_train_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Rescale the weights to be between 0 and 255</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2d469ac7d61edc4bc58cab334a8918c22941a01441de10b6d4765c4c10c2d274.png" src="_images/2d469ac7d61edc4bc58cab334a8918c22941a01441de10b6d4765c4c10c2d274.png" />
</div>
</div>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Link to this heading">¶</a></h3>
<p>We are NOT going to use the stochastic version of the gradient descent method. Therefore, we set the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#classifier = Softmax() # using Softmax</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">SVM</span><span class="p">()</span>     <span class="c1"># using linear SVM</span>
<span class="n">loss_hist</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">verbose_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration 0 / 2000: loss 16.092469
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration 1000 / 2000: loss 0.276174
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration 1999 / 2000: loss 0.217232
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration number&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d6252d6e6826307688252b3677d1807d7e032e66e5be3e5450353dc00f59dd86.png" src="_images/d6252d6e6826307688252b3677d1807d7e032e66e5be3e5450353dc00f59dd86.png" />
</div>
</div>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_train_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y_train</span> <span class="o">==</span> <span class="n">Y_train_pred</span><span class="p">),</span> <span class="p">))</span>

<span class="n">Y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;testing accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y_test</span> <span class="o">==</span> <span class="n">Y_test_pred</span><span class="p">),</span> <span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>training accuracy: 0.956796
testing accuracy: 0.925760
</pre></div>
</div>
</div>
</div>
</section>
<section id="save-the-matrix">
<h3>Save the matrix<a class="headerlink" href="#save-the-matrix" title="Link to this heading">¶</a></h3>
<p>All we need is only the <code class="docutils literal notranslate"><span class="pre">W</span></code> matrix. We need to store the flattended version of the matrix, thus, we add a comma at every end of a line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">&#39;W.txt&#39;</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%.8f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Copy this matrix to the Arduino PROGMEM, inside a file named <code class="docutils literal notranslate"><span class="pre">matrix.h</span></code>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "py.3.12.4"
        },
        kernelOptions: {
            name: "py.3.12.4",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'py.3.12.4'</script>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="intro.html">
              <img class="logo" src="_static/logo.png" alt="Logo of Project name not set"/>
            </a></p>
  <div>
    <h3><a href="intro.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Demo-1: Linear Classifiers with Arduino</a><ul>
<li><a class="reference internal" href="#arduino-code-repository">Arduino Code repository</a></li>
<li><a class="reference internal" href="#preparations">Preparations</a><ul>
<li><a class="reference internal" href="#the-svm-class">The SVM class</a></li>
<li><a class="reference internal" href="#the-softmax-class">The Softmax class</a></li>
</ul>
</li>
<li><a class="reference internal" href="#usps-dataset">USPS Dataset</a><ul>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#testing">Testing</a></li>
<li><a class="reference internal" href="#save-the-matrix">Save the matrix</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="cnn.html"
                          title="previous chapter">Convolutional Neural Network with Pre-Convolved Input</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="arduino_mlp_classifier.html"
                          title="next chapter">Demo-2: Two-Layer Fully Connected Network with Arduino</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/arduino_linear_classifier.ipynb"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="arduino_mlp_classifier.html" title="Demo-2: Two-Layer Fully Connected Network with Arduino"
             >next</a> |</li>
        <li class="right" >
          <a href="cnn.html" title="Convolutional Neural Network with Pre-Convolved Input"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="intro.html">AEK3LBB3</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Demo-1: Linear Classifiers with Arduino</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2025.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    </div>
  </body>
</html>