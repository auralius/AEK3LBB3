{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f4f274",
   "metadata": {},
   "source": [
    "# Two-Layer Fully Conected Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a231062",
   "metadata": {},
   "source": [
    "**This material is heavily based on the popular Standford CS231n lecture material.** [Please check on their website for more detailed information](https://cs231n.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d94f3",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "As usual, let's start with some preparations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0e94322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "from utils import *\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518019ce",
   "metadata": {},
   "source": [
    "## Diagram of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510957f",
   "metadata": {},
   "source": [
    "![](./images/2fcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b761c5e",
   "metadata": {},
   "source": [
    "## class TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17b45e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet():\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "\n",
    "    In other words, the network has the following architecture:\n",
    "\n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(W1, b1, W2, b2, X, Y, reg=0.0, grad=False):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - Y: Vector of training labels. Y[i] is the label for X[i], and each y[i] is\n",
    "        an integer in the range 0 <= Y[i] < C. This parameter is optional; if it\n",
    "        is not passed then we only return scores, and if it is passed then we\n",
    "        instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "        - grad: flag to or NOT to return the loss gradients\n",
    "\n",
    "        Returns:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "        samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "        with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        fc1 = X.dot(W1) + b1     # fully connected\n",
    "        X2 = np.maximum(0, fc1)  # ReLU\n",
    "        F = X2.dot(W2) + b2 # fully connected\n",
    "        \n",
    "        # Compute the loss \n",
    "        F = F - np.max(F, axis=1).reshape(-1,1)\n",
    "        expF = np.exp(F)\n",
    "        softmax = expF/np.sum(expF, axis=1).reshape(-1,1) \n",
    "        loss = np.sum(-np.log(softmax[range(N),Y])) / N + reg  * (np.sum(W2 * W2) + np.sum( W1 * W1 ))\n",
    "        \n",
    "        if grad == True: # loss gradient is optionals\n",
    "            # Backward pass: compute gradients\n",
    "            softmax[np.arange(N) ,Y] -= 1\n",
    "            softmax /= N\n",
    "\n",
    "            # W2 gradient\n",
    "            dW2 = X2.T.dot(softmax)   # [HxN] * [NxC] = [HxC]\n",
    "\n",
    "            # b2 gradient\n",
    "            db2 = softmax.sum(axis=0)\n",
    "\n",
    "            # W1 gradient\n",
    "            dW1 = softmax.dot(W2.T)   # [NxC] * [CxH] = [NxH]\n",
    "            dfc1 = dW1 * (fc1>0)      # [NxH] . [NxH] = [NxH]\n",
    "            dW1 = X.T.dot(dfc1)       # [DxN] * [NxH] = [DxH]\n",
    "\n",
    "            # b1 gradient\n",
    "            db1 = dfc1.sum(axis=0)\n",
    "\n",
    "            # regularization gradient\n",
    "            dW1 += reg * 2 * W1\n",
    "            dW2 += reg * 2 * W2\n",
    "\n",
    "            dW = np.hstack((dW1.flatten(), db1, dW2.flatten(), db2))\n",
    "        \n",
    "            return (loss, dW)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self, X, Y, reg=1e-5, gtol=1e-5, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "        X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "\n",
    "        D, H = self.params['W1'].shape\n",
    "        H, C = self.params['W2'].shape\n",
    "\n",
    "        def obj(x):\n",
    "            W1 = x[0: D*H].reshape(D,H)\n",
    "            b1 = x[D*H: D*H+H]\n",
    "            W2 = x[D*H+H: D*H+H+(H*C)].reshape(H,C)\n",
    "            b2 = x[D*H+H+(H*C):]\n",
    "\n",
    "            loss = self.loss(W1, b1, W2, b2, X, Y, reg=reg, grad=True)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            if verbose == True:\n",
    "                print(loss)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        x0 = np.hstack((self.params['W1'].flatten(), \n",
    "                        self.params['b1'], \n",
    "                        self.params['W2'].flatten(), \n",
    "                        self.params['b2']))\n",
    "        res = minimize(obj, x0, method='L-BFGS-B', jac=True, options={'gtol': gtol, 'disp': True})\n",
    "\n",
    "        self.params[\"W1\"] = res.x[0: D*H].reshape(D,H)\n",
    "        self.params[\"b1\"] = res.x[D*H: D*H+H]\n",
    "        self.params[\"W2\"] = res.x[D*H+H: D*H+H+(H*C)].reshape(H,C)\n",
    "        self.params[\"b2\"] = res.x[D*H+H+(H*C):]\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "        classify.\n",
    "\n",
    "        Returns:\n",
    "        - Y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "        the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "        to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "\n",
    "        # Compute the forward pass\n",
    "        fc1 = X.dot(W1) + b1     # fully connected\n",
    "        X2 = np.maximum(0, fc1)  # ReLU\n",
    "        scores = X2.dot(W2) + b2 # fully connected\n",
    "    \n",
    "        y_pred = np.argmax( scores, axis=1)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd388947",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a67d11",
   "metadata": {},
   "source": [
    "## Breast Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb6eeb",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e44c6",
   "metadata": {},
   "source": [
    "### Load training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "080758d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension numbers : 10\n",
      "Number of data    : 569\n",
      "Labels            : [0 1]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"./datasets/breast_cancer/wdbc.data\", delimiter=\",\", dtype=str)\n",
    "X = np.float32(data[:, 2:12])  # 10 dimensions\n",
    "\n",
    "# Diagnosis (M = malignant, B = benign)\n",
    "Y = np.zeros(X.shape[0], dtype=np.int32) \n",
    "Y[np.where(data[:,1]=='M')] = 1\n",
    "Y[np.where(data[:,1]=='B')] = 0\n",
    "\n",
    "print(\"Dimension numbers :\", X.shape[1])\n",
    "print(\"Number of data    :\", X.shape[0])\n",
    "print(\"Labels            :\", np.unique(Y))\n",
    "\n",
    "# For the NN\n",
    "input_size  = X.shape[1]\n",
    "num_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "475b1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:400, :]\n",
    "Y_train = Y[0:400]\n",
    "X_test  = X[401:, :]\n",
    "Y_test  = Y[401:]\n",
    "\n",
    "num_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a075f6",
   "metadata": {},
   "source": [
    "### Setup the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d279df0",
   "metadata": {},
   "source": [
    "* `hidden_size = 100`\n",
    "* `learning_rate = 1e-4`\n",
    "* `reg = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fa03682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy   :  0.9225\n",
      "Validation accuracy :  0.9166666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33707/769074464.py:72: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(-np.log(softmax[range(N),Y])) / N + reg  * (np.sum(W2 * W2) + np.sum( W1 * W1 ))\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 20\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats = net.train(X_train, Y_train, reg=0.001, gtol=0.001, verbose=False)\n",
    "\n",
    "# Predict on the validation set\n",
    "train_acc = (net.predict(X_train) == Y_train).mean()\n",
    "print('Training accuracy   : ', train_acc)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_test) == Y_test).mean()\n",
    "print('Validation accuracy : ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e8df5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac336f",
   "metadata": {},
   "source": [
    "## Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ba83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension numbers : 64\n",
      "Number of data    : 3823\n",
      "Labels            : [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"./datasets/handwritten_digits/optdigits.tra\", delimiter=\",\", dtype=int)\n",
    "X_train = np.int32(data[:, 0:-1])  \n",
    "Y_train = np.int32(data[:, -1]) \n",
    "\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train - X_train - mean_image\n",
    "\n",
    "print(\"Dimension numbers :\", X_train.shape[1])\n",
    "print(\"Number of data    :\", X_train.shape[0])\n",
    "print(\"Labels            :\", np.unique(Y_train))\n",
    "\n",
    "# For the NN\n",
    "input_size  = X_train.shape[1]\n",
    "num_classes = len(np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./datasets/handwritten_digits/optdigits.tes\", delimiter=\",\", dtype=int)\n",
    "X_test = np.int32(data[:, 0:-1])  \n",
    "Y_test = np.int32(data[:, -1]) \n",
    "\n",
    "X_test = X_test - mean_image\n",
    "\n",
    "print(\"Dimension numbers :\", X_test.shape[1])\n",
    "print(\"Number of data    :\", X_test.shape[0])\n",
    "print(\"Labels            :\", np.unique(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "309dddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats = net.train(X_train, Y_train, reg=0.01, gtol=1e-3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8b48e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy :  0.9937222076902956\n",
      "Test accuracy     :  0.910962715637173\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "train_acc = (net.predict(X_train) == Y_train).mean()\n",
    "print('Training accuracy : ', train_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "test_acc = (net.predict(X_test) == Y_test).mean()\n",
    "print('Test accuracy     : ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel 3.12.4",
   "language": "python",
   "name": "py.3.12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
